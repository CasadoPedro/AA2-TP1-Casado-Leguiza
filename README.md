# Clasificador de Gestos: Piedra, Papel o Tijeraâœ‹âœŠâœŒï¸

## ğŸ“Œ DescripciÃ³n

Este proyecto implementa un sistema de clasificaciÃ³n de gestos para el juego "Piedra, Papel o Tijeras", utilizando visiÃ³n por computadora y aprendizaje automÃ¡tico. La detecciÃ³n de la mano se realiza mediante **MediaPipe**, y la clasificaciÃ³n se realiza con una **red neuronal densa (MLP)** entrenada sobre coordenadas de puntos clave (landmarks) de la mano.

El proyecto estÃ¡ dividido en tres partes funcionales:

1. GrabaciÃ³n de dataset (`record-dataset.py`)
2. Entrenamiento del modelo (`train-gesture-classifier.py`)
3. ClasificaciÃ³n en tiempo real (`rock-paper-scissors.py`)


## ğŸ¯ Objetivos

- Detectar gestos de mano usando **MediaPipe Hand Landmarker**
- Capturar y almacenar un dataset etiquetado de gestos
- Entrenar un clasificador utilizando una red neuronal
- Realizar inferencia en tiempo real con la cÃ¡mara web


## ğŸ§  TecnologÃ­as Utilizadas

- Python 3.10+
- [MediaPipe](https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker)
- OpenCV
- TensorFlow / Keras
- NumPy
- Scikit-learn


## ğŸ“ Estructura de Archivos
```
â”œâ”€â”€ /scripts/  
    â””â”€â”€  record-dataset.py    
    â””â”€â”€  train-gesture-classifier.py
    â””â”€â”€  rock-paper-scissors.py  
â”œâ”€â”€ /data/  
       â””â”€â”€ dataset.npy  
â”œâ”€â”€ /models/  
       â””â”€â”€ gesture_classifier.h5  
â””â”€â”€ /img-ejecucion/  
       â””â”€â”€ capturas_del_funcionamiento.png
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md 
```

## ğŸ§ª Scripts

### 1. `record-dataset.py`

Captura imÃ¡genes desde la cÃ¡mara web, detecta landmarks con MediaPipe y guarda coordenadas `x,y` de 21 puntos (42 valores) mÃ¡s su etiqueta (`0`: piedra, `1`: papel, `2`: tijera) en un archivo `.npy`.

**Uso:**

```bash
python record-dataset.py piedra
```

### ğŸ§ª 2. `train-gesture-classifier.py` â€” Entrenamiento del Modelo

Este script entrena una red neuronal sobre los datos de landmarks capturados con MediaPipe. Separa el dataset en caracterÃ­sticas (`X`) y etiquetas (`y`), realiza codificaciÃ³n one-hot de las clases y entrena usando validaciÃ³n con `EarlyStopping`.

#### ğŸ§¬ Arquitectura del modelo:

- Entrada: `42` neuronas (21 puntos x 2 coordenadas)
- Capas ocultas:
  - `Dense(128, relu)`
  - `Dense(64, relu)`
  - `Dense(32, relu)`
- Salida: `3` neuronas (`softmax`) para clasificar entre piedra, papel o tijera.

#### ğŸ“‚ Dataset de entrada:

Debe estar guardado en `../data/dataset.npy` y debe haber sido generado previamente con `record-dataset.py`.

#### ğŸ’¾ Modelo entrenado:

Se guarda como: `../models/gesture_classifier.h5`

#### â–¶ï¸ CÃ³mo usar:

```bash
python train-gesture-classifier.py
```

### ğŸ® 3. `rock-paper-scissors.py` â€” Clasificador en Tiempo Real

Este script permite reconocer en tiempo real los gestos de "piedra", "papel" o "tijera" utilizando una imagen capturada desde la cÃ¡mara web y un modelo previamente entrenado.

#### âš™ï¸ Funcionamiento

1. Se abre la cÃ¡mara web y se muestra la imagen en pantalla.
2. Al presionar la tecla `c`, se captura una imagen.
3. Se procesan los landmarks de la mano utilizando **MediaPipe Hands**.
4. Se extraen los 21 puntos `(x, y)` y se convierten en un vector de 42 valores.
5. El modelo entrenado (`gesture_classifier.h5`) predice el gesto.
6. El resultado se muestra por consola como `"piedra"`, `"papel"` o `"tijera"`.

#### â–¶ï¸ CÃ³mo usar

```bash
python rock-paper-scissors.py
```

## âš ï¸ Advertencias

- Este proyecto fue desarrollado y probado con **Python 3.11**.  
  El comportamiento podrÃ­a variar si se utiliza una versiÃ³n diferente de Python, especialmente en lo que respecta a dependencias como `MediaPipe`, `TensorFlow` y `OpenCV`.

- La configuraciÃ³n de la cÃ¡mara estÃ¡ seteada por defecto a **640x480** pÃ­xeles:
  ```python
  cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
  cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
  ```
  Esto fue elegido porque era la resoluciÃ³n disponible en el entorno de desarrollo, pero puede ser ajustado segÃºn las capacidades de la cÃ¡mara del usuario.
